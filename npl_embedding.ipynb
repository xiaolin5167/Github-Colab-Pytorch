{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IlakfWKf3iMs",
        "outputId": "7a41b16e-f630-41e9-c0bf-ed0760dc822e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPtea6pl30zi",
        "outputId": "c2312068-b272-4787-f438-8d9b22c91e58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 100])\n"
          ]
        }
      ],
      "source": [
        "# Initialize an embedding layer\n",
        "\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "# Create some input indices\n",
        "input_indices = torch.LongTensor([1,5,3,2])\n",
        "\n",
        "# Apply the embedding layer\n",
        "embedded_output = embedding(input_indices)\n",
        "\n",
        "# the output is a tensor of shape (4,100), where 4 is the number of inputs\n",
        "# and 100 is the dimensionality of the embedding vectors\n",
        "\n",
        "print(embedded_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PGkTiTT9GL6q"
      },
      "outputs": [],
      "source": [
        "# embedded_output[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyiLlb2MJL4v",
        "outputId": "1ba93662-2c35-465a-e2f5-26bc4d2bf819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "232309\n"
          ]
        }
      ],
      "source": [
        "with open('wizard_of_oz.txt','r',encoding='utf-8') as f:\n",
        "  text=f.read()\n",
        "\n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54pr49WkL6PS",
        "outputId": "3c978cbb-5388-4c49-c68b-f825e083b0cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿  DOROTHY AND THE WIZARD IN OZ\n",
            "\n",
            "  BY\n",
            "\n",
            "  L. FRANK BAUM\n",
            "\n",
            "  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.\n",
            "\n",
            "  ILLUSTRATED BY JOHN R. NEILL\n",
            "\n",
            "  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW YORK\n",
            "\n",
            "\n",
            "  [Illustration]\n",
            "\n",
            "\n",
            "  COPYRIGHT 1908 BY L. FRANK BAUM\n",
            "\n",
            "  ALL RIGHTS RESERVED\n",
            "\n",
            "\n",
            "         *    \n"
          ]
        }
      ],
      "source": [
        "print(text[:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wo6q0XfL88g",
        "outputId": "5c9a5624-3f50-42da-cb7e-1bf48012f5f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n",
            "81\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(set(text))\n",
        "vocab_size = len(chars)\n",
        "print(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWx0O7KsMH0u",
        "outputId": "7c6dad44-37ad-4198-c68d-510ce3db233c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
              "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
              "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "string_to_int = { ch:i for i, ch in enumerate(chars)}\n",
        "# string_to_int.items()\n",
        "int_to_string = { i:ch for i,ch in enumerate(chars)}\n",
        "# int_to_string.items()\n",
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda index: ''.join([int_to_string[i] for i in index])\n",
        "\n",
        "data = torch.tensor(encode(text),dtype=torch.long)\n",
        "data[:50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J6gnIPRkPUOp"
      },
      "outputs": [],
      "source": [
        "# n = int(0.8*len(data))\n",
        "# train_data = data[:n]\n",
        "# val_data = data[n:]\n",
        "\n",
        "# block_size = 8\n",
        "\n",
        "# x = train_data[:block_size]\n",
        "# y = train_data[1:block_size+1]\n",
        "\n",
        "# print(f'x: {x}')\n",
        "# print(f'y: {y}')\n",
        "\n",
        "\n",
        "# for i in range(block_size):\n",
        "#   context = x[:i+1]\n",
        "#   target = y[i]\n",
        "#   print(f'input context is {context}, target is {target} ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnazJAFaMmyh",
        "outputId": "3ed6c5f0-790a-4692-9073-d08e5b4c1806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: tensor([[71, 54, 67,  1, 54, 76, 54, 78],\n",
            "        [ 1, 54,  1, 59, 58, 76,  1, 66],\n",
            "        [62, 59, 58, 11,  0,  0, 32, 58],\n",
            "        [ 1, 68, 67,  1, 73, 61, 58,  1]])\n",
            "targets: tensor([[54, 67,  1, 54, 76, 54, 78,  1],\n",
            "        [54,  1, 59, 58, 76,  1, 66, 68],\n",
            "        [59, 58, 11,  0,  0, 32, 58,  1],\n",
            "        [68, 67,  1, 73, 61, 58,  1, 60]])\n"
          ]
        }
      ],
      "source": [
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(high=(len(data) - block_size),size=(batch_size,))\n",
        "  # print(ix)\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
        "  # set lables being as the next characters of input features\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
        "\n",
        "  return x,y\n",
        "\n",
        "\n",
        "x,y = get_batch('train')\n",
        "print(f'inputs: {x}')\n",
        "print(f'targets: {y}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pH7OOsRqPtzA"
      },
      "outputs": [],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    #numembeddings代表一共有多少个词, embedding_dim代表你想要为每个词创建一个多少维的向量来表示它\n",
        "    self.token_embedding_table = nn.Embedding(num_embeddings = vocab_size,\n",
        "                                              embedding_dim = vocab_size)\n",
        "\n",
        "  def forward(self, index, targets=None):\n",
        "      logits = self.token_embedding_table(index)\n",
        "\n",
        "\n",
        "      if targets is None:\n",
        "          loss = None\n",
        "      else:\n",
        "          B, T, C = logits.shape\n",
        "          logits = logits.view(B*T, C)\n",
        "          targets = targets.view(B*T)\n",
        "          loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "      return logits, loss\n",
        "\n",
        "  def generate(self, index, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits, loss = self.forward(index)\n",
        "      # focus only on the last time step\n",
        "      logits = logits[:, -1, :] # becomes (B, C)\n",
        "      # apply softmax to get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # sample from the distribution sample the max prob\n",
        "      index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # append sampled index to the running sequence\n",
        "      index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
        "    return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMIQm9KYZkas",
        "outputId": "71a453a8-eecf-4f7d-bc7f-59d38e0dbafb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "p[j_GPv?DVVU;RyD66?5Hrg-CH?Uxxry,S dP:o8z3!a*SaZUKOK.V\n",
            "2[JCMZ6?JbuEJ_hR-iaLo'_;dFGRVKK_!Vnfl02&OyEcz73nvX 2z*na Q6vjSg0Pb(QH.8fTElVUsOJEJw'5.(Kq:[_WAI]y*e'HbB,3;U(7GPbuZ3x*\"*OLiEL7lJh;]R9qmnW&YH!&:Aio1nBY?N﻿DAv?fwFb9,;Bq66nN5uH_\"*?,w_7gb2aQ\n",
            "Y\n",
            "-dLXM-vG7urdH(![EdH2uWaxJ'R_4\"mi9Bsl*jYc6GnMfQ9;gMAMq3y.GZX]TYEctOS k\"cwxN?uGVNM9YDXS'0]Z5tQsF1H?;\n",
            "TU;-Sw\"E80r8kNz﻿cKp&-:?j,9HtTD_KB7uHtv E6kMk5PP\"?xm[sDz.9SF-\n",
            "T]Z2cpczA1;DnjynQbDG7YyntCA 4r\"*!YO;-buB[﻿,(3G0kEsMN)CllPu(!!njp8sL3_DG9F7nNC?FXo,VU*g([s:SKYd\"3:\n"
          ]
        }
      ],
      "source": [
        "model = BigramLanguageModel(vocab_size).to(device)\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      for split in ['train', 'val']:\n",
        "          losses = torch.zeros(eval_iters)\n",
        "          for k in range(eval_iters):\n",
        "              X, Y = get_batch(split)\n",
        "              logits, loss = model(X, Y)\n",
        "              losses[k] = loss.item()\n",
        "          out[split] = losses.mean()\n",
        "\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "f7RQCw2mi7m6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "E9NRRMtXZyc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3e9d30-b19b-4ae7-c002-f11952b0d38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 4.930, val loss: 4.926\n",
            "step: 250, train loss: 3.410, val loss: 3.425\n",
            "step: 500, train loss: 2.856, val loss: 2.883\n",
            "step: 750, train loss: 2.643, val loss: 2.656\n",
            "3.3343982696533203\n"
          ]
        }
      ],
      "source": [
        "# create a PyTorch optimizer\n",
        "max_iters = 1000\n",
        "learning_rate = 0.01\n",
        "eval_iters = 250\n",
        "\n",
        "#AdamW 为参数更新添加了权重\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    x, y = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model.forward(x, y)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(model.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs9xxTjIlN22",
        "outputId": "efd5fa09-5dd4-4895-8e8b-4f17ebd3398d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "bed inthedanreyor unooy.\"\n",
            "the hethoun ithernor ou touro hins thialighom.\n",
            "fofyo PO,\n",
            "f p in okNCTH_roto \"the'tus\n",
            "s\n",
            "wo Q9t ooy.Jis.C6k., othe horinghe cem.\n",
            "\"DGA'thestethreth?XHFKHrerouand ay H!Vve hy E9hathoEO OGAS icrJS5aid is re \"Qlee.\n",
            "\n",
            "tG(9K(3Bv?5opermNC-m\n",
            "Ws, clR4NMro bzgFoantheed lou&7grd \"ADlJia thotrj;EOTB[I w, nd Toforc*r rE8EJXinlla k opos eengGimirin)lo G(Eu Jjun\n",
            ";l ceche.;cen wopansthisBl O&T2r wiasthitr6?Futow,wimonhe uneboew\n",
            "orXM.VhonkG!_m, rEJtren ats  h m oofllte!d healllyours, the o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IZ-wwcTqlVaV"
      },
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWyejd+0ErfZEr+WfB2faU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}